[[enduser]]
== *EndUser* role
:toc: macro
include::{partialsdir}/header-macros.adoc[]
include::{partialsdir}/header-uri.adoc[]
include::{partialsdir}/mso4sc-uri.adoc[]

TIP: When running into some strange portal behavior, first try to `logout/login` from the portal
before reporting an issue.

In the Canary Portal, an *EndUser* can only:

* <<running, run>> application in *Experiments*

[IMPORTANT]
====
Some apps are freely available for all portal users.
To use these apps, you can skip the purchase step.
Right now all {hifimagnet} apps are freely available.

Before going to the {uri-msoportal-experiment-dev-www}[*Experiments*]
it is mandatory to connect at least to {uri-msoportal-market-www}[*Marketplace*] 
and to the {uri-msoportal-ckan-www}[*Data Catalogue*].

To use and create private datasets for storing inputs data and results,
you have to upload your *Data Catalogue key* into the Experiments *settings*
as described <<api_key, here>>.
====

If you ever have need to access *Data Catalogue* or *Market Place*
use the {uri-msoportal-www}[main MSOPortal]

[[running]]
=== Running an {hifimagnet} application in *Experiments*

image::CanaryMSO4SC_Exp_User.png[*Experiments* *Instances* main menu for *EndUSer*]

* Go to {uri-msoportal-experiment-dev-www}[*Experiments*]
* Select *Add a new instance*
** define an ID
** select a registered app in the sliding menu

At this point you should have a page with all the options that need to be filled.

image::CanaryMagThel.png[ex: {hifimagnet} app `MagTherm`]

The fields to fill may be grouped into categories:

* specs for running simulations
* HPC settings
* specs retrieving singularity images
* specs for singularity image

The *EndUser* shall only need to fill the fields related to the 1st and 2nd category.
The fields that are to be filled by *EndUser* are marked in *bold*.

The others fields are designed for more advanced users.


[NOTE]
====
.List of keys to be defined for being notified by email (only available in {hifimagnet})
[options="header,footer"]
|===
| Key                        | Description              | Default          | Notes
| *email_user*               | email address            |                  |
|===

//| *email_type*               | type of notifications    |                  | workload_manager dependant (for `slurm`: `ALL,END,FAIL`)

.List of keys specs for running simulations
[options="header,footer"]
|===
| Key                           | Description                                 | Default                                                            | Notes
| *Dataset resource: model*     | dataset mode                                | `None`                                                             | comes from the Data Catalog
| *Output dataset: outputs_at*  |                                             |                                                                    |
| *cfgfile*                     | configuration file                          | `/usr/share/doc/hifimagnet/ThermoElectricModel/quarter-turn3D.cfg` | cfg files need to be in the container, or in a mounted directory or in the dataset
|===

.List of keys to be defined for HPC settings
[options="header,footer"]
|===
| Key                        | Description                                   | Default                                         | Notes
| *HPC: primary*             | HPC settings                                  | "cesga"                                         |
| resource_hpc_modules       | modules to load depending on the HPC machine  | `[gcc/6.3.0, openmpi/2.0.2, singularity/2.4.2]` | depends on HPC and singularity image
| resource_hpc_basedir       | volume where instance is created              | `${LUSTRE}`                                     | depends on HPC and singularity image
| hpc_feelpp                 | volume where results will be stored              | `${LUSTRE}/feel`                                     | depends on HPC and app
| resource_hpc_volumes       | volumes to be mounted on the HPC machine      | `[/scratch,/mnt,${LUSTRE}/feel:/feel]`          | depends on HPC and singularity image
| *resource_hpc_partition*   | select partition queue                        | 'thin-shared'                                   | shall depends on batch system and HPC
| resource_hpc_reservation   | select reservation queue                      | ``                                              | optional
| *resource_parallel_tasks*  | number of tasks/processes to run in parallel  | 2                                               | depends on batch system and selected partition
| *resource_max_time*        | maximum allowed time                          | '00:30:00'                                      | hours, minutes and seconds
|===

.List of keys specs for retrieving singularity images
[options="header,footer"]
|===
| Key                        | Description                                    | Default                        | Notes
| sregistry_storage          | path to container directory                    | `${LUSTRE}/singularity_images` |
|===

// .List of keys specs for retrieving singularity images
// [options="header,footer"]
// |===
// | Key                        | Description                                    | Default                        | Notes
// | sregistry_client           | define default sregistry client                |  `registry`                    |
// | sregistry_client_secrets   | path to file where sregistry secret are stored | `$HOME/.sregistry`             |
// | sregistry_storage          | path to container directory                    | `${LUSTRE}/singularity_images` |
// | sregistry_url              | URI pointing to the sregistry                  | `sregistry.srv.cesga.es`       |
// | sregistry_image            | URI pointing to the sregistry-cli image        | `mso4sc/sregistry`             |
// |===

// .List of keys specs for singularity image
// [options="header,footer"]
// |===
// | Key                        | Description              | Default          | Notes
// | singularity_image_uri      |  URI pointing to the singularity image   | `hifimagnet/hifimagnet:stretch` |
// | singularity_image_filename |  Filename of the singularity image       | `hifimagnet-stretch.simg` |
// | singularity_image_cleanup  | force remove of singularity image        | `false` |
// |===

====

[IMPORTANT]
====
HPC settings for local {lncmi} ressources are specificied xref::lncmi-hpc.adoc[here].
====

Once filled, click on *Create New Instance*.

TIP: While creating a new app instance, any error will result in a pop-up message.


The next step is to launch the app:

* select the instance *ID*, you defined in the previous step,
* click on *Run*, wait until the button in blue turns green

You can check the application logs, if any where defined, by clicking on the *Application Logs* button.
To go back to the main view, just click on *execution logs*

image::CanaryMagThel_monitoring.png[ex: Application logs for {hifimagnet} app `MagTherm`]

[IMPORTANT]
====
The result are stored in `<resource_hpc_basedir>/<hpc_feelpp>`. Make sure that `<hpc_volumes>` bindmount list option contains this field: eg ``<resource_hpc_basedir>/<hpc_feelpp>:/feel` on cesga where `<hpc_feelpp>=$LUSTRE/feel`.

As a best practice, we **highly recommend** the user to set `<hpc_feelpp>` to a specific directory for each instance.

It means that the actual simulation results are stored in:

* `<resource_hpc_basedir>/<hpc_feelpp>/hifimagnet/ThermoElectricModel/<geofile>/thermoelectric-linear_V1T1_N1/np_<parallel_tasks>` for MagThel app.

`<geofile>` correspond to the input with the same name in the <cfgfile>.

When running the instance *ID*:

* a directory `<resource_hpc_basedir>/<workdir_prefix>_<date>_<id>` will be created (`<id> is a random number);
* the dataset will eventually be downloaded and extracted into this directory;
* the script `<job_prefix><sid>.script` will be created and submitted to the queueing system of the HPC ressource.

Upon completion two files will be created in the directory:

* `<job_prefix><sid>.out`: contains the output of the script,
* `<job_prefix><sid>.err`: contains some log and errors if any.

A red button will indicates an error.
In this event, check the output of the script (aka `slurm-<id>.out`, the `.err` and `.out` files).
To do so you need to have `<workdir_keep>` set to `true`.
====

Finaly to destroy your instance *ID* , just click on:

* *Destroy Instance!*
* or *Force Destroy*

This will remove your instance *ID* from the instance list.

See this video for a live demo.

video::_tB2aLV-_So[youtube]

[[running_dataset]]
=== Running an {hifimagnet} application in *Experiments* using datasets from *DataCatalogue*

Basically the procedure is the same as <<running, above>>. The only difference here is that you need:

* to select a dataset in the *DataSet* sliding menu
* to select a ressource within the *DataSet* 
* to select an output dataset in the *Output DataSet* sliding menu
* to enter the name of the `<cfgfile>` you will use

You should at the end have something like in the following image.

image::portal_running_dataset.png[Selecting a dataset]


[NOTE]
====
The dataset, at least those from {hifimagnet} organization in the *Data Catalogue* are
gzipped archive that holds all the files needed to run:

* a mesh or cad file
* a `<cfgfile>`
* a list of `json` files for physical properties and/or boundary conditions
* eventually some more files when using some {magnettools} features (eg ...)

The format of the files in a dataset are respectively described in:

* for {hifimagnet} simulation 
* for mesh and cad format
* for {magnettools}

====


[[RemoteDesktop]]
=== Running an Remote Desktop resource in *Visualization*

* Go to *Visualization*
* Click *Desktop Available*
** Select your *Infrastructure* in the sliding menu

NOTE: If no *Remote desktop* is defined, you will receive a warning message. Create a xref::index.adoc#post[*Setting*], then go back to this step.

** Click on *Create*
** Click either on *Desktop* or *View Only*

It should start a web page within your browser.
In this page you should be able to:

* start a 'terminal' and check the output of your simulation

The outputs of the job (ie `slurm` jobs) are stored in the `<hpc_basedir>/<hpc_workdir_prefix>_<date>_<id>`.
The actual results for the simulation would be stored in `<hpc_feelpp>` directory following {feelpp} usage.


* start a post-processing GUI:
** either by loading {uri-paraview-www}[{paraview}]
** or using singularity {uri-ensight-www}[{ensight}] image
* to quit just click on `Menu/logout` (top left)

For more details on using {paraview} or {ensight} please check this xref:post:index.adoc#post_manual[section].
See the video bellow for a live demo.

video::yr27O_Ll9jk[youtube]

[NOTE]
====
You can also perform some pre-processing GUI with {salome} {hifimagnet} plugin. See xref:cad:index.adoc#salome_on_remotedesktop[here] for details.
====

[[offering_setup]]
== To go further

Available apps in **MSO4SC Portal** are:

* MagnetTools: xref::bmap.adoc[MagMapAxi], 
* Preprocessing: xref::MagCAD.adoc[MagCAD], xref::MagMesh.adoc[MagMesh]
* Simulations:
** direct models: xref::MagThel.adoc[MagThel], xref::MagFull.adoc[MagFull], xref::3Dsim.adoc[MagSim]
** CRB:
// * Postprocessing:

To get more details about the app, click on it.
This will display more information about the app and its settings.

